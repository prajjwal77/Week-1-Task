import axios from "axios";
import Chunk from "../models/Chunk.js";
import { generateEmbedding } from "./embedding.service.js";
import { cosineSimilarity } from "../utils/cosine.js";

const OLLAMA_URL = "http://localhost:11434";
const LLM_MODEL = "phi3:mini";

export async function askQuestion(question) {
  // 1) Embed the question
  const qEmbedding = await generateEmbedding(question);

  // 2) Fetch chunks with embeddings
  const chunks = await Chunk.find({ embedding: { $exists: true, $ne: [] } });

  // 3) Score by cosine similarity
  const scored = chunks
    .map(c => ({
      chunk: c,
      score: cosineSimilarity(qEmbedding, c.embedding)
    }))
    .sort((a, b) => b.score - a.score)
    .slice(0, 3); // top-3 context

  // 4) Build context + citations
  const context = scored
    .map(
      (s, i) =>
        `[${i + 1}] (${s.chunk.filename}, page ${s.chunk.page}) ${s.chunk.text}`
    )
    .join("\n\n");

  // 5) Ask LLM with grounding
  const prompt = `
You are an enterprise SOP assistant.
Answer ONLY using the provided context.
If not found, say: "No relevant answer found in the provided documents."

Context:
${context}

Question:
${question}

Answer with citations like [1], [2].
`;

 const llm = await axios.post(
  `${OLLAMA_URL}/api/generate`,
  {
    model: LLM_MODEL,
    prompt,
    stream: false
  },
  {
    headers: { "Content-Type": "application/json" }
  }
);



  // 6) Return answer + sources
  return {
    answer: llm.data.response.trim(),
    sources: scored.map((s, i) => ({
      ref: i + 1,
      filename: s.chunk.filename,
      page: s.chunk.page
    }))
  };
}
